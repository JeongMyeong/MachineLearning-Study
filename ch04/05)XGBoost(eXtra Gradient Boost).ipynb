{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "- 트리 기반의 앙상블 학습에서 가장 각광받고 있는 알고리즘 중 하나\n",
    "- 분류에 있어서 일반적으로 다른 머신러닝보다 뛰어난 예측성능을 보임\n",
    "- GBM 기반이지만 GBM의 단점인 느린 수행 시간 및 과적합 규제 부재 등의 문제를 해결하여 각광받음.\n",
    "- 주요 장점\n",
    "        분류와 회귀에서 뛰어난 예측 성능을 보임\n",
    "        GBM에 비해 빠른 수행 성능을 보장\n",
    "        과적합 규제 기능으로 과적합에 좀 더 강한 내구성을 가짐\n",
    "        자체 내장된 교차 검증\n",
    "        결손값을 자체 처리한다. --> 결측값을 자체적으로 처리?????????\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  XGBoost 파이퍼 파라미터 유형\n",
    "- 일반 파라미터\n",
    "        스레드의 개수나 silent모드 등\n",
    "- 부스터 파라미터\n",
    "        트리 최적화, 부스팅, 규제 등과 관련 파라미터 등을 지칭\n",
    "- 학습 태스크 파라미터\n",
    "        학습 수행 시의 객체 함수, 평가를 위한 지표 등을 설정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <일반 파라미터>\n",
    "\n",
    "##### booster\n",
    "        gbtree(tree based model), gblinear(linear model) 두개의 옵션\n",
    "        디폴트는 gbtree\n",
    "##### silent \n",
    "        디폴트는 0\n",
    "        출력 메시지 나타내기 싫을 때 1\n",
    "##### nthread\n",
    "        CPU 실행 스레드 개수 조정\n",
    "        디폴트는 CPU 전체 스레드 다 사용\n",
    "\n",
    "### <부스터 파라미터>\n",
    "##### eta [default=0.3, alias:learning_rate]\n",
    "        GBM의 학습률과 같은 파라미터\n",
    "        0에서 1값\n",
    "        디폴트는 0.3\n",
    "        보통은 0.01 ~ 0.2 사이 값을 선호\n",
    "##### num_boost_rounds \n",
    "        GBM의 n_estimators와 같은 파라미터\n",
    "##### min_child_weight[default=1]\n",
    "        GBM의 min_child_leaf와 유사\n",
    "        과적합을 조절 하기위해 사용\n",
    "##### gamma[default=0, alias : min_split_loss]\n",
    "        트리의 리프노드를 추가적으로 나눌지를 결정하는 최소 손실 감소 값.\n",
    "        해당 값 보다 큰 손실이 감소된 경우 리프노드를 분리\n",
    "        값이 클수록 과적합 감소 효과\n",
    "##### max_depth[default=6]\n",
    "        트리 기반 알고리즘의 max_depth와 같다.\n",
    "        0으로 지정하면 깊이 제한이 없다.\n",
    "        값이 크면 과적합 될 가능성이 높다.\n",
    "        보통 3 ~ 10 사이 값을 적용\n",
    "##### sub_sample(default=1]\n",
    "        GBM의 subsample과 동일\n",
    "        데이터를 샘플링하는 비율을 지정.\n",
    "        0.5로 지정하면 전체 데이터의 절반을 트리를 생성하는데 사용.\n",
    "        일반적으로 0.5 ~ 1 사이 값을 사용\n",
    "##### colsample_bytree[default=1]\n",
    "        GBM의 max_features와 유사.\n",
    "        트리 생성에 필요한 피처를 임의로 샘플링하는 데 사용\n",
    "        많은 피처가 있는 경우 과적합을 조정하는데 사용\n",
    "##### lambda [default=1, alias:reg_lambda]\n",
    "        L2 규제 적용 값.\n",
    "        피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과\n",
    "##### alpha [default=0, alias:reg_alpha]\n",
    "        L1 규제 적용 값\n",
    "        피처 개수가 많을 경우 적용을 검토하며 값이 클수록 과적합 감소 효과\n",
    "##### scale_pos_weight [default=1]\n",
    "        특정 값으로 치우친 비대칭한 클래스로 구성된 데이터 세트의 균형을 유지\n",
    "### <학습 태스크 파라미터>\n",
    "##### objective\n",
    "        최솟값을 가져야할 손실 함수를 정의\n",
    "##### binary:logistic\n",
    "        이진 분류일 때 적용\n",
    "##### multi:softmax\n",
    "        다중 분류일 때 적용\n",
    "        따로 num_class 파라미터를 지정해야한다.\n",
    "##### multi:softprob\n",
    "        multi:softmax와 유사하나 개별 에측확률을 반환\n",
    "##### eval_metric\n",
    "        검증에 사용되는 함수를 정의\n",
    "        기본값은 rmse\n",
    "        분류일 경우에 error\n",
    "        - rmse, mae, logloss, error, merror, mlogloss, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 뛰어난 알고리즘일수록 파라미터 튜닝 필요가 적다.\n",
    "- 튜닝한거에 비행 성능 향상 효과가 대부분 높지는 않다.\n",
    "- 피처의 수가 매우 많거나 피처 간 상관되는 정도가 마낳거나 데이터 세트에 다라 여러 가지 특성이 있을 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과적합 문제 해결 방법\n",
    "- eta 값을 낮춘다. (0.01 ~ 0.1) eta 값을 낮추는 경우 num_round는 반대로 높여줘야 한다.\n",
    "- max_depth 값을 낮춘다.\n",
    "- min_child_weight 값을 높인다.\n",
    "- gamma 값을 높인다.\n",
    "- subsample과 colsample_bytree 를 조정하는것도 트리가 복잡하게 생성되는것을 막아 오버피팅을 해결할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost는 자체적으로 교차검증, 성능 평가, 피처 중요도 등의 시각화 기능을 가지고 있다.\n",
    "- XGBoost는 기본 GBM 과는 다르게 조기 종료 기능이 있다.\n",
    "- 예를 들어 n_estimators를 200으로 설정 후 조기 중단 값을 50 으로 설정하면 1에서 200 회 까지 부스팅을 반복하다가 50회를 반복하는 동안 학습 오류가 감소하지 않으면 더 이상 부스팅을 진행하지 않고 종료한다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
