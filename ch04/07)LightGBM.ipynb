{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < LightGBM >  \n",
    "- XGBoost와 함께 부스팅 계열 알고리즘에서 가장 각광 받고 있음.\n",
    "- XGBoost는 뛰어난 부스팅 알고리즘이지만 여전히 학습시간이 오래걸림\n",
    "- LightGBM의 가장 큰 장점은 XGB보다 학습시간이 훨씬 적다. 그리고 메모리 사용량로 적다.\n",
    "- LightGBM의 단점은 적은 데이터 세트에 적용할 경우 과적합이 발생하기 쉽다. 일반적으로 10,000건 이하의 데이터가 적은 데이터세트의 기준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < LightGBM Concept >\n",
    "- GBM 계열의 트리 분할 방법과 다르게 리프 중심 트리 분할 방식을 사용\n",
    "- 대부분의 트리 기반 알고리즘은 균형 트리 분할 방식을 사용함 최대한 균형 잡힌 트리를 유지하면서 분할하기 때문에 트리의 깊이가 최소화될 수 있다.\n",
    "- 리프 중심 트리 분할 방식은 트리의 균형을 맞추지 않고 최대 손실 값을 가지는 리프노드를 분할하면서 트리의 깊이가 깊어지고 비대칭 적인 규칙 트리가 생성.\n",
    "- 최대 손실값을 가지는 리프노드를 지속적으로 분할해 생성된 규칙 트리는 학습을 반복할수록 결국 균형 트리 분할 방식보다 예측 오류 손실을 최소화 할 수 있다. 는것이 LightGBM의 구현 사상.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < LightGBM의 하이퍼파라미터 >\n",
    "- LightGBM은 XGB와 다르게 리프노드가 계속 분할되면서 트리의 깊이가 깊어지므로 트리 특성에 맞는 하이퍼 파라미터 설정이 필요.\n",
    "### <주요 파라미터>\n",
    "##### num_iterations [default = 100]\n",
    "        반복 수행 하려는 트리의 개수\n",
    "        사이킷런에서는 n_estimators\n",
    "##### learning_rate [default = 0.1]\n",
    "        0에서 1사이 값을 지정하여 부스팅 스텝을 반복적으로 수행할 때 업데이트되는 학습률\n",
    "        일반적으로 n_estimators를 크게하고 learning_rate를 작게해서 예측 성능을 향상 시키지만 과적합 문제와 학습시간이 길어지는 것도 고려해야함\n",
    "##### max_depth [default = -1]\n",
    "        트리 기반 알고리즘음 max_depth와 같다.\n",
    "        0보다 작은 값을 지정하면 깊이제한 없음\n",
    "        LightGBM은 Leaf Wise 기반이므로 다른 트리 기반 모델모다 깊이가 깊다.\n",
    "##### num_leaves [default = 31]\n",
    "        하나의 트리가 가질 수 있는 최대 리프 개수\n",
    "##### boosting [default = gbdt]\n",
    "        부스팅의 트리를 생성하는 알고리즘을 적는다.\n",
    "        gbdt : 일반적인 그래디언트 부스팅 \n",
    "        rf : 랜덤포레스트\n",
    "##### bagging_fraction [default = 1.0]\n",
    "        트리가 커져 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율\n",
    "        사이킷런 래퍼 LightGBM 에서 sub_sample로도 사용 함\n",
    "##### feature_fraction [default = 1.0]\n",
    "        개별 트리를 학습할 때 무작위로 선택하는 피처의 비율\n",
    "        과적합 방지 목적\n",
    "        사이킷런에는 colsample_bytree로도 사용함\n",
    "##### lambda_l2 [default = 0.0]\n",
    "        L2 규제 \n",
    "        값이 클수록 과적합 감소\n",
    "        사이킷런에서는 reg_lambda로 사용\n",
    "##### lambda_l1 [default = 0.0]\n",
    "        L1 규제\n",
    "        사이킷런에서는 reg_alpha로 사용\n",
    "### <Learning Task 파라미터>\n",
    "##### objective\n",
    "        최솟값을 가져야 할 손실함수 정의\n",
    "        회귀, 다중 클래스 분류, 이진분류에 따라 손실함수 지정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### < 하이퍼 파라미터 튜닝 방안>\n",
    "- learning_rate를 작게하면서 n_estmators를 크게 하는 것은 부스팅 계열 튜닝에서 가장 기본적인 튜닝 방안이다.\n",
    "- n_estimators를 너무 크게하는 건 과적합 가능성이 크다.\n",
    "- 이 밖에 과적합을 방지하기 위해서 L1,L2 규제를 적용하거나 피처 개수나 레코드 개수를 줄이기 위해 feature_fraction(colsample_bytree), bagging_fraction(subsample) 파라미터를 적용 시킨다.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
